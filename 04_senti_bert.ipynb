{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip -q install keras_bert\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npath_dict = {}\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path_dict[filename] = os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.\npath_dict","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(path_dict['train.csv'])\ntest_df = pd.read_csv(path_dict['test.csv'])\ntrain_df.shape, test_df.shape, train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = test_df.copy()\n\ntrain_df['Review Title'] = train_df['Review Title'] + ' '\ntrain_df['text'] = 3*train_df['Review Title'] + train_df['Review Text']\n\ntest_df['Review Title'] = test_df['Review Title'] + ' '\ntest_df['text'] = 3*test_df['Review Title'] + test_df['Review Text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_multi_label = train_df.groupby('text')['topic'].apply(lambda x: '|'.join(x)).reset_index()\nprint('Total combinations of multi-labels in the train using the 21 classes:', \n      len(train_multi_label['topic'].value_counts().to_dict()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_multi_label = test_df.groupby('text').count().reset_index()\ntest_multi_label['review_count'] = test_multi_label['Review Text']\ntest_multi_label = test_multi_label[['text', 'review_count']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdf = test_df.merge(test_multi_label[['text', 'review_count']], how='left', on = 'text')\nmdf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_multi_label.shape, test_multi_label.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm_notebook().pandas()\n\ndef preprocess_text(text):\n    from nltk.corpus import stopwords\n    stopwords = set(stopwords.words('english'))\n\n    sent = decontracted(text)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9:]+', ' ', sent)\n    # https://gist.github.com/sebleier/554280\n    \n    words = sent.split(' ')\n    words = [word.lower() for word in words]\n    sent = ' '.join(e for e in words if e not in stopwords)\n    return sent.lower().strip()\n\ntrain_multi_label['text'] = train_multi_label['text'].progress_apply(lambda x: preprocess_text(x))\ntest_ml_text = test_multi_label['text'].progress_apply(lambda x: preprocess_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_val = train_multi_label[['text']].copy()\nfrom sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\nlb = MultiLabelBinarizer()\ny_train_val = lb.fit_transform(train_multi_label['topic'].apply(lambda x:x.split('|')))\nnum_classes = len(lb.classes_)\nnum_classes, X_train_val.shape, y_train_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat = []\ncat_feat = []\ntext_feat = ['text']\n\nfeatures = num_feat + cat_feat + text_feat\nlabel = 'topic'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_path = 'uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')\n\nimport codecs\nfrom keras_bert import load_trained_model_from_checkpoint\n\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# df_train, df_val, = train_test_split(train_df, stratify = train_df[label],\n#                                     test_size = 0.1, random_state = 2019)\n# df_train.shape, df_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom keras_bert import Tokenizer\nfrom keras.utils import to_categorical\n\ntokenizer = Tokenizer(token_dict)\n\ndef load_data_from_df(X,y, maxlen = 256):\n    global tokenizer\n    indices, labels = [], []\n    for text, label in zip(X['text'].tolist(), y):\n        ids, segments = tokenizer.encode(text, max_len=maxlen)\n        indices.append(ids)\n        labels.append(label)\n    items = list(zip(indices, labels))\n    np.random.shuffle(items)\n    indices, labels = zip(*items)\n    indices = np.array(indices)\n    return [indices, np.zeros_like(indices)], y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtv, ytv = load_data_from_df(X_train_val, y_train_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_x, train_y = load_data_from_df(df_train)\n# val_x, val_y = load_data_from_df(df_val)\n\n# from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n# lb = LabelBinarizer()\n# train_y = lb.fit_transform(train_y)\n# val_y = lb.transform(val_y)\n# num_classes = len(lb.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 256\nBATCH_SIZE = 16\nEPOCHS = 9\nLR = 1e-4\n\nmodel = load_trained_model_from_checkpoint(config_path,checkpoint_path, training=True,\n                                           trainable=True,seq_len=SEQ_LEN,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# @title Build Custom Model\nimport keras\nfrom keras_bert import AdamWarmup, calc_train_steps\n\ninputs = model.inputs[:2]\ndense = model.get_layer('NSP-Dense').output\noutputs = keras.layers.Dense(units=num_classes, activation='sigmoid')(dense)\n\ndecay_steps, warmup_steps = calc_train_steps(y_train_val.shape[0],\n                                             batch_size = 16,\n                                             epochs = 9,)\n\nmodel = keras.models.Model(inputs, outputs)\nmodel.compile(AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n              loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'],)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# @title Initialize Variables\nimport tensorflow as tf\nimport keras.backend as K\n\nsess = K.get_session()\nuninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\ninit_op = tf.variables_initializer(\n    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n)\nsess.run(init_op)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(Xtv, ytv, epochs = 3, batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(token_dict)\nmaxlen = 256\nindices = []\nfor text in test_ml_text.tolist():\n    ids, segments = tokenizer.encode(text, max_len=maxlen)\n    indices.append(ids)\nindices = np.array(indices)\nx_test = [indices, np.zeros_like(indices)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_probas = model.predict(x_test)\ny_test_labels = lb.inverse_transform(y_test_probas > 0.1)\ntest_predictions = ['|'.join(pr) for pr in y_test_labels]\nprint(len(x_test), len(test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_6_predictions = []\nfor proba in y_test_probas:\n    top_6_predictions.append(lb.classes_[np.argsort(proba)[-6:]])  # from back","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_row_preds = []\nfor count, prediction in zip(test_multi_label['review_count'].values, top_6_predictions):\n    row_prediction = '|'.join(prediction[-count:])\n    test_row_preds.append(row_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds(test_multi_label):\n    df1 = test_multi_label.merge(test_multi_label.topics.str.split('|',expand=True),\n                    left_index=True, right_index=True, how='outer')\n    df1.drop('topics',axis=1,inplace=True)\n    df2 = df1.melt(['text'], value_vars = [0, 1, 2, 3, 4, 5])\n    res_df = df2[df2['value'].isin(lb.classes_)]\n    return res_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_multi_label['topics'] = test_row_preds\nres_df = get_preds(test_multi_label)\nprint(res_df.shape, res_df.columns)\nres_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape, res_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_pred_df = test_df.merge(res_df[['text', 'value']], how='left', on = 'text')\nres_pred_df.shape, res_pred_df.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_pred_df = res_pred_df.drop_duplicates().reset_index(drop = 1)\nres_pred_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df['topic'] = res_pred_df['value']\nprint(pred_df.shape, pred_df.columns)\npred_df['topic'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '6-amazon-reviews-mlbert.csv'\nprint(len(test_df), len(pred_df))\npred_df.to_csv(filename, index = None)\nfrom IPython.display import FileLink\nFileLink(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # res_pred = res_pred_df[~(res_pred_df[['text', 'value']].duplicated())]\n# # res_pred.shape\n# pred_df['topic'] = res_pred['value']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# res_pred_df = res_pred_df['text','values']'.drop_duplicates().reset_index(drop = 1)\n# res_pred_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import accuracy_score, f1_score\n\n# print(\"Accuarcy:\", accuracy_score(val_y, y_val_pred>0.5))\n# print(\"F1:\", f1_score(val_y, (y_val_pred>0.5).astype(int), average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_test_pred = model.predict(test_x)\n# test_predictions = lb.inverse_transform(y_test_pred)\n\n# # for i in range(0, len(test_df)):\n# #     test_predictions.append(np.argmax(y_test_pred[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# file = '3-amazon_bert_base.csv'\n# pred_df['topic'] = test_predictions\n# pred_df.to_csv(file, index = None)\n# from IPython.display import FileLink\n# FileLink(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n# for thresh in np.arange(0.1, 0.501, 0.01):\n#     thresh = np.round(thresh, 2)\n#     print(\"F1 score at threshold {0} is {1}\".format(thresh, f1_score(val_y,(pred_val_y>thresh).astype(int))))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}